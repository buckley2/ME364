{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_8b1qaWYy8"
      },
      "source": [
        "# K-Nearest Neighbors\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [**Introduction**](#Intro)   \n",
        "2. [**Data Normalization**](#NormStand)\n",
        "3. [**Model Development Procedure**](#ModelDev)\n",
        "4. [**Binary Classification**](#BinaryClass)\n",
        "\n",
        "    4.1 [**Model Development**](#BinModelDev)\n",
        "\n",
        "    4.2 [**Model Evaluation**](#BinModelEval)\n",
        "\n",
        "5. [**Multiclass Classification**](#MultiClass)\n",
        "\n",
        "    5.1 [**Model Development**](#MultModelDev)\n",
        "\n",
        "    5.2 [**Model Evaluation**](#MultModelEval)\n",
        "  \n",
        "6. [**Final Comments**](#FinCom)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbs0W9BFXxqF"
      },
      "source": [
        "## 1 Introduction <a name=\"Intro\"></a>\n",
        "\n",
        "To implement the nearest neighbor rule, we need to first quantify the similarity between the two points. There are different ways to measure similarity, or equivalently dissimilarity. Here is how this algorithm works:\n",
        "\n",
        "1. Pick a value for k\n",
        "2. Calculate the distance of unknown case from all cases.\n",
        "3. Select the k points in the training data that are \"nearest\" to the unknown data point.\n",
        "4. Make a prediction using the most popular target variable class from the k-nearest neighbors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78FsG8AI4GGv"
      },
      "source": [
        "## 2 Data Normalization <a name=\"NormStand\"></a>\n",
        "\n",
        "Many modeling algorithms perform better when numerical input variables are scaled to a standard range.\n",
        "\n",
        "The most popular technique for scaling numerical data prior to modeling is normalization. Normalization scales each input variable separately to the range 0-1. You can noarmalize your data using: $x_{norm} = \n",
        "\\frac{x – x_{min}}{x_{max} – x_{min}}$\n",
        "\n",
        "You can normalize your variable using the scikit-learn module <font color='blue'>MinMaxScaler</font>.\n",
        "\n",
        "<font color='orange'>__Note__</font>: There are multiple methods for normalization and scaling the data. You can see a list of these methods and how they affect the data here: \n",
        "\n",
        "Article: https://drive.google.com/file/d/19alt-Le4m6SQKFURmT8gt5Rfr0JIbAZp/view?usp=sharing\n",
        "\n",
        "Code: https://www.kaggle.com/discdiver/guide-to-scaling-and-standardizing/notebook \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnDOZF6uk_e8"
      },
      "source": [
        "## 3 Model Development Procedure <a name=\"ModelDev\"></a>\n",
        "\n",
        "Followings are the steps to develop KNN model using <font color='blue'>scikit-learn</font> library:\n",
        "\n",
        "__1.__ Import Numpy, KNeighborsClassifier, MinMaxScaler, and train_test_split funcions from <font color='blue'>scikit_learn</font> library:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler  # For normalization\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "__2.__ Define dependant variables (target variable) and independent variables (features) from data set:\n",
        "```python\n",
        "x_data=np.array(df[['feature1','feature2','feature3',...]])\n",
        "y_data=df['target variable']\n",
        "```\n",
        "\n",
        "__3.__ Normalize your data using <font color='blue'>MinMaxScaler</font> (Optional but advised)\n",
        "```python\n",
        "MinMaxscaler = MinMaxScaler()  # define min max scaler object\n",
        "x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data\n",
        "```\n",
        "\n",
        "__4.__ Split the data into train and test sets: `x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data, test_size=[szie of the test data])`\n",
        "\n",
        "__5.__ Create the KNN object using the constructor with known value of k: `neigh = KNeighborsClassifier(n_neighbors = k)`\n",
        "\n",
        "__6.__ Use the fit function to fit the model to the training data: `neigh.fit(x_train,y_train)`\n",
        "\n",
        "__7.__ Then, make prediction using the test data and training data:\n",
        "```python\n",
        "yhatTest = neigh.predict(x_test)\n",
        "yhatTrain= neigh.predict(x_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV_kFf0mxT1Q"
      },
      "source": [
        "As alwasy, there needs to be a little bit of an exploratory data analysis at the beginning. One step that might be useful to do is to look at the target variable to see how many categories do we have and how balanced the data is in terms of the target variable. This can be done using\n",
        "```python\n",
        "# To see the list of unique values for our target variables.\n",
        "df['target variable'].unique()\n",
        "# To see the distribution of the target variable in terms of different categories. \n",
        "# This gives you bar chart showing the distribution.\n",
        "df['target variable'].value_counts().plot(kind='barh')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjah7M7ZygRo"
      },
      "source": [
        "We are going to use part of the Fuel Economy Dataset, which is produced by the Office of Energy Efficiency and Renewable Energy of the U.S. Department of Energy. Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. This dataset can be accessed from here: https://github.com/MasoudMiM/ME_364/blob/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv and a description of the data is provided at this link: https://www.fueleconomy.gov/feg/EPAGreenGuide/GreenVehicleGuideDocumentation.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnPO4NXfWSZg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = ('https://raw.githubusercontent.com/MasoudMiM/ME_364/main/EPA_Green_Vehicle_Guide/EPA_2020_Fuel_Economy.csv')\n",
        "df = pd.read_csv(url)           \n",
        "\n",
        "df.drop(columns='Unnamed: 0', inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b5b9blOylWj"
      },
      "source": [
        "Check to see if we have any missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQXAOD1Wy96j"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpIlK4jl4nul"
      },
      "source": [
        "## 4 Binary Classification <a name=\"BinaryClass\"></a>\n",
        "\n",
        "\n",
        "If the target variable has two possible values (classes), the classification is a binary classification. Here, we first develop a model to classify the cars based on their fuel type based on City MPG and Comb MPG. Since Fuel type in this dataset only has two possible classes, `Gasoline` and `Diesel`, so this is a binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge1YtRB25T8K"
      },
      "source": [
        "### 4.1 Model Development <a name=\"BinModelDev\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK2X-v260avP"
      },
      "source": [
        "Step __1__, importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu8_JlM8zeUo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler  # For normalization\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEDxzAai0fMJ"
      },
      "source": [
        "Step __2__, selecting our features and target variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6-acu1i0Pqm"
      },
      "outputs": [],
      "source": [
        "x_data=np.array(df[['City MPG','Comb CO2']])   # Feature variables\n",
        "y_data=df['Fuel']                              # Target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHiHt-OHyqxK"
      },
      "source": [
        "Let's also take a look at the possible values for our target variable as well as the distribution of the category. This helps us find out how imbalanced our dataset is in terms of target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaPPqj88y8-H"
      },
      "outputs": [],
      "source": [
        "df['Fuel'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVpj_Bj7zC3m"
      },
      "source": [
        "There are two types of fuels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQoxZAbRz-WP"
      },
      "outputs": [],
      "source": [
        "print('Target variable distribution:')\n",
        "print( df['Fuel'].value_counts() )\n",
        "\n",
        "df['Fuel'].value_counts().plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVVtIHOczJgZ"
      },
      "source": [
        "Our data is imbalanced. We have much more data representing the cars with Gasoline fuel type than the cars using Diesel. This will affect your model development and prediction. To see how to deal with imbalanced data, see this: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndVXPsJH0m3f"
      },
      "source": [
        "Step __3__, normalizing our feature variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdVPbO6S1uSx"
      },
      "outputs": [],
      "source": [
        "MinMaxscaler = MinMaxScaler()  # define min max scaler\n",
        "x_data_scaled = MinMaxscaler.fit_transform(x_data)  # transform data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8pmJaKb3KqA"
      },
      "source": [
        "Before we go further, let's take a look at our normalized features and compare that to original features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeAkPg8H3TDn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure(figsize=(15,6))\n",
        "fig.add_subplot(1,2,1)\n",
        "plt.scatter(x_data[:,0],x_data[:,1])\n",
        "plt.xlabel('City MPG'),plt.ylabel('Comb CO2')\n",
        "plt.title('Original Data')\n",
        "\n",
        "fig.add_subplot(1,2,2)\n",
        "plt.scatter(x_data_scaled[:,0],x_data_scaled[:,1])\n",
        "plt.xlabel('City MPG'),plt.ylabel('Comb CO2')\n",
        "plt.title('Normalized Data');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-PbSgua7Q7c"
      },
      "source": [
        "Step **4**, splitting our data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W6hKJ1Y-Ikk"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x_data_scaled,y_data,test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYwOdVEx7oxo"
      },
      "source": [
        "Step __5__ and __6__, creating KNN classifier and fit the model to training data. Here we used _euclidean_ distance but you can choose any distance method from the list mentioned in the documentation page: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kvg6VVR-LQn"
      },
      "outputs": [],
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors = 4, metric='euclidean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAsJEu4b-OXw"
      },
      "outputs": [],
      "source": [
        "neigh.fit(x_train,y_train) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bllG_WM18RVa"
      },
      "source": [
        "Step __7__, making predictions using test data and training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYLTREov-TU4"
      },
      "outputs": [],
      "source": [
        "yhatTest = neigh.predict(x_test)\n",
        "yhatTrain= neigh.predict(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgFQwBt8ByAN"
      },
      "source": [
        "<font color='orange'>__Note__:</font> Keep in mind that if you want to make a prediction for a specific value, you need to scale the value first before using your model for prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByoDd00KCG53"
      },
      "source": [
        "### 4.2 Model Evaluation <a name=\"BinModelEval\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv8Fqm-pD4sX"
      },
      "source": [
        "We can evaluate the performance of our KNN classifier using Jaccard index, F-score, or accuracy. We can also generate the confusion matrix to have a better sense of how the model performs in terms of TP, TN, FP, and FN.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qAdqOMnCIr9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TujuvRB7EMhf"
      },
      "outputs": [],
      "source": [
        "acc_scoreTrain = accuracy_score(y_train,yhatTrain)\n",
        "acc_scoreTest = accuracy_score(y_test,yhatTest)\n",
        "print(f'accuracy score for training data is {acc_scoreTrain:0.3f} and accuracy score for test data is {acc_scoreTest:0.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA4BjX0ZDFCo"
      },
      "outputs": [],
      "source": [
        "J_scoreTrain = jaccard_score(y_train,yhatTrain, average='micro')\n",
        "J_scoreTest = jaccard_score(y_test,yhatTest, average='micro')\n",
        "print(f'Jaccard index for training data is {J_scoreTrain:0.3f} and Jaccard index for test data is {J_scoreTest:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7__eRTbjaYqW"
      },
      "outputs": [],
      "source": [
        "F_scoreTrain = f1_score(y_train,yhatTrain, average='micro')\n",
        "F_scoreTest = f1_score(y_test,yhatTest, average='micro')\n",
        "print(F_scoreTrain,F_scoreTest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZ4roKeG79P"
      },
      "source": [
        "You can also calculate _precision_ and _recall_ using their corresponding functions. Look at here https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html and here https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8FHFYoPDUmF"
      },
      "outputs": [],
      "source": [
        "print('Confusion matrix for training data')\n",
        "CM_scoreTrain = confusion_matrix(y_train,yhatTrain)   # possible option normalize='true'\n",
        "print(CM_scoreTrain)\n",
        "\n",
        "print(40*'-')\n",
        "\n",
        "print('Confusion matrix for test data')\n",
        "CM_scoreTest = confusion_matrix(y_test,yhatTest)   # possible option normalize='true'\n",
        "print(CM_scoreTest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqVIBmAs166W"
      },
      "source": [
        "Looking at the plots of confusion matrix for training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ZEocwuFwGC"
      },
      "outputs": [],
      "source": [
        "dispTr = ConfusionMatrixDisplay(CM_scoreTrain,display_labels=neigh.classes_)\n",
        "dispTr.plot()\n",
        "\n",
        "dispTs = ConfusionMatrixDisplay(CM_scoreTest,display_labels=neigh.classes_)\n",
        "dispTs.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw5sT4Zc5WRk"
      },
      "source": [
        "We used k=4, however, we can run the model for different values of k and see if there is a better value for k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQIQm61SHL4J"
      },
      "outputs": [],
      "source": [
        "Ks = 25\n",
        "Jacc_Test = np.zeros((Ks-1))\n",
        "Jacc_Train= np.zeros((Ks-1))\n",
        "\n",
        "F_Loop_Test = np.zeros((Ks-1))\n",
        "F_Loop_Train= np.zeros((Ks-1))\n",
        "\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    neighLoop = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n",
        "    yhatTestLoop  = neighLoop.predict(x_test)\n",
        "    yhatTrainLoop = neighLoop.predict(x_train)\n",
        "    Jacc_Test[n-1] = jaccard_score(y_test, yhatTestLoop, average='micro')\n",
        "    Jacc_Train[n-1] = jaccard_score(y_train, yhatTrainLoop, average='micro')\n",
        "\n",
        "    F_Loop_Test[n-1] = f1_score(y_test, yhatTestLoop, average='micro')\n",
        "    F_Loop_Train[n-1] = f1_score(y_train, yhatTrainLoop, average='micro')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE5RJkxuKKl3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1,Ks),Jacc_Test,'r-o',range(1,Ks),Jacc_Train,'g-o')\n",
        "plt.legend(['Test Data', 'Train Data'])\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('Jaccard Index');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1,Ks),F_Loop_Test,'r-o',range(1,Ks),F_Loop_Train,'g-o')\n",
        "plt.legend(['Test Data', 'Train Data'])\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('F-score');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEZOvQJBZRxP"
      },
      "source": [
        "## 5 Multiclass Classification <a name=\"MultiClass\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLWUHoZJ3bZY"
      },
      "source": [
        "Let's develop the model using a target variable that has more than two possible values. In this case, we use `SmartWay`, which can be 'Yes', 'No', or 'Elite'. This is a multi-class classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXd3NIndajME"
      },
      "source": [
        "### 5.1 Model Development <a name=\"MultModelDev\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM5Vk_EZ3t2K"
      },
      "outputs": [],
      "source": [
        "df['SmartWay'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSMcQo53zpP"
      },
      "source": [
        "Looking at the distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQHVSdPu36Sz"
      },
      "outputs": [],
      "source": [
        "print('Target variable distribution:')\n",
        "print( df['SmartWay'].value_counts() )\n",
        "\n",
        "df['SmartWay'].value_counts().plot(kind='barh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9hYE303_aY"
      },
      "source": [
        "Let's develop the model following the same steps as before, this time the target variable has three possible classes. Therefore, this will be a multiclass classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z5kry-VZwm3"
      },
      "outputs": [],
      "source": [
        "x_dataM=np.array(df[['City MPG','Hwy MPG']])\n",
        "y_dataM=df['SmartWay']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boEbVr1raAh1"
      },
      "outputs": [],
      "source": [
        "MinMaxscalerM = MinMaxScaler()  # define min max scaler\n",
        "x_data_scaledM = MinMaxscaler.fit_transform(x_dataM)  # transform data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSCFsrkZaGnF"
      },
      "outputs": [],
      "source": [
        "x_trainM,x_testM,y_trainM,y_testM=train_test_split(x_data_scaledM,y_dataM,test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Q9utemaQEl"
      },
      "outputs": [],
      "source": [
        "neighM = KNeighborsClassifier(n_neighbors = 4, metric='euclidean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77-ntfPHaWRE"
      },
      "outputs": [],
      "source": [
        "neighM.fit(x_trainM,y_trainM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl0ii4vsabV0"
      },
      "outputs": [],
      "source": [
        "yhatTestM = neighM.predict(x_testM)\n",
        "yhatTrainM= neighM.predict(x_trainM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erufQ-FealQU"
      },
      "source": [
        "### 5.2 Model Evaluation <a name=\"MultModelEval\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWaVXYVaYUM"
      },
      "outputs": [],
      "source": [
        "J_scoreTrainM = jaccard_score(y_trainM,yhatTrainM, average='micro')\n",
        "J_scoreTestM = jaccard_score(y_testM,yhatTestM, average='micro')\n",
        "print(J_scoreTrainM,J_scoreTestM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zJOAjMLawB_"
      },
      "outputs": [],
      "source": [
        "F_scoreTrainM = f1_score(y_trainM,yhatTrainM, average='micro')\n",
        "F_scoreTestM = f1_score(y_testM,yhatTestM, average='micro')\n",
        "print(F_scoreTrainM,F_scoreTestM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyZQBNFRa4V7"
      },
      "outputs": [],
      "source": [
        "CM_scoreTrainM = confusion_matrix(y_trainM,yhatTrainM)   # possible option normalize='true'\n",
        "CM_scoreTestM = confusion_matrix(y_testM,yhatTestM)   # possible option normalize='true'\n",
        "\n",
        "dispTrM=ConfusionMatrixDisplay(CM_scoreTrainM, display_labels=neighM.classes_)\n",
        "dispTrM.plot()\n",
        "dispTsM=ConfusionMatrixDisplay(CM_scoreTestM, display_labels=neighM.classes_) \n",
        "dispTsM.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWaXdM-La-wD"
      },
      "outputs": [],
      "source": [
        "Ks = 25\n",
        "Jacc_TestM = np.zeros((Ks-1))\n",
        "Jacc_TrainM= np.zeros((Ks-1))\n",
        "\n",
        "F_Loop_TestM = np.zeros((Ks-1))\n",
        "F_Loop_TrainM= np.zeros((Ks-1))\n",
        "\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    neighLoopM = KNeighborsClassifier(n_neighbors = n).fit(x_trainM,y_trainM)\n",
        "    yhatTestLoopM  = neighLoopM.predict(x_testM)\n",
        "    yhatTrainLoopM = neighLoopM.predict(x_trainM)\n",
        "    Jacc_TestM[n-1] = jaccard_score(y_testM, yhatTestLoopM, average='micro')\n",
        "    Jacc_TrainM[n-1] = jaccard_score(y_trainM, yhatTrainLoopM, average='micro')\n",
        "\n",
        "    F_Loop_TestM[n-1] = f1_score(y_testM, yhatTestLoopM, average='micro')\n",
        "    F_Loop_TrainM[n-1] = f1_score(y_trainM, yhatTrainLoopM, average='micro')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1,Ks),Jacc_TestM,'r-o',range(1,Ks),Jacc_TrainM,'g-o')\n",
        "plt.legend(['Test Data', 'Train Data'])\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('Jaccard Index');\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1,Ks),F_Loop_TestM,'r-o',range(1,Ks),F_Loop_TrainM,'g-o')\n",
        "plt.legend(['Test Data', 'Train Data'])\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('F-score');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91RnJJ3uNneX"
      },
      "source": [
        "## 6 Final Comments <a name=\"FinCom\"></a>\n",
        "\n",
        "In this notebook, we learned how to develop a KNN classifier and use that to make predictions. We also learned about data normalization by scaling the data into [0,1] range. Finally, some functions that can be used to evaluate a classifier were implemented. The main aspect in developing a KNN classifier is to find the appropriate value for _k_.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "11_KNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}